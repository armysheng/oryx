# This file specifies all configuration parameters that can be used by Oryx.
#
# An application will typically override only a few of these. An application specifies parameter
# values by creating a config file, like myapp.conf, and then running the Computation Layer or
# Serving Layer with "java -Dconfig.file=myapp.conf ..."
#
# It is typically easiest to use simple properties file-style key-value syntax when writing a .conf
# file -- many lines of "key=value" pairs, one on each line.
#
# There are only two parameters that must be specified in any config file -- the type of model to build,
# and the location where input/output files will be stored. For example:
#
# model=${als-model}
# model.instance-dir=/your/location
#
# The rest of this file defines all parameters that can be set, and their default values.
# Note that a property like...
#
# foo = {
#   bar = 1
# }
#
# ... is set by "foo.bar=1" in key-value syntax.

# generic-api
# These parameters are common to serving-layer.api and computation-layer.api.
# You will not set values under generic-api directly. Instead, set for example "serving-layer.api.port=8080"
# to configure the port parameter, defined below, for the Serving Layer. "computation-layer.api.port" can be
# set, similarly.

generic-api = {

  # The port that the service is listening on. This is overriden in usages below.
  port = null
  # The port that the service is listening on when using HTTPS. This is overridden in usages below.
  secure-port = null

  # User name for connecting to the service, if required. If set, must be set with password.
  # If enabled, this will enable HTTP DIGEST authentication in the API.
  user-name = null
  # Password for connecting to the service, if required. If set, must be set with user-name.
  # If enabled, this will enable HTTP DIGEST authentication in the API.
  password = null

  # The keystore file containing the server's SSL keys. Only necessary when
  # accessing a server with temporary self-signed key, which is not trusted
  # by the Java SSL implementation.
  keystore-file = null
  # Password needed for keystore file above, if any
  keystore-password = null
}

# serving-layer
# Configuration parameters for the Serving Layer.

serving-layer = {
  # Inherits and overrides generic-api above
  api = ${generic-api} {
    # Default to use well-known HTTP port for Serving Layer
    port = 80
    # Default to use well-known HTTPS port for Serving Layer
    secure-port = 443

    # If true, operations that set or modify data, like /ingest, are not available
    read-only = false

    # An optional prefix for the path under which the service is deployed. For
    # example, set to "/contextPath" to expose services at paths like "http://example.org/contextPath/..."
    context-path = /
  }

  # Optional. The local directory used for reading input, writing output, and storing
  # user input and model files in local mode. It is used for staging input for upload in distributed mode.
  local-input-dir = /tmp/Oryx

  # Optional. Name of an implementation of RescorerProvider to use to rescore recommendations and similarities,
  # if any. The class must be added to the server classpath. This may also be specified as a comma-separated
  # list of class names, in which case all will be applied, in the given order.
  # This only applies to als-model at the moment.
  rescorer-provider-class = null

  # Optional. Name of an implementation of CandidateFilter to use. See CandidateFilter javadoc.
  # This only applies to als-model at the moment.
  candidate-filter-class = null
}

# computation-layer
# Configuration parameters for the Computation Layer

computation-layer = {
  # Inherits and overrides generic-api above
  api = ${generic-api} {
    # Default to use common secondary HTTP port for Computation Layer
    port = 8080
    # Default to use common secondary HTTPS port for Computation Layer
    secure-port = 8443
  }

  # Max degree of parallelism (really, number of reducers or threads)
  # "auto" to try to guess the best value
  parallelism = "auto"

  # Memory requested for mappers' and reducers' Hadoop containers, in MB
  mapper-memory-mb = 1024
  reducer-memory-mb = 1024

  # Multiple of memory required per worker (really, reducer), in MB, for high-memory steps
  # Default to 3x the M/R default.
  worker-high-memory-factor = 3
}


# model
# Parameters common to all models.

model = {

  # Whether to run the model locally, without Hadoop
  local-computation = false
  # Whether to run the model on data in the local filesystem. Can't be true if local-computation is false
  local-data = false

  # What type of model is this: ${als-model}, ${kmeans-model}, ${rdf-model}
  type = null

  # After this amount of time has elapsed, in minutes, without rebuilding a model in the Computation Layer,
  # trigger a new build. Set to -1 to never trigger a model rebuild due to time elapsed.
  time-threshold = -1

  # After this amount of data has been written, in MB, into the Computation Layer without rebuilding a model,
  # trigger a new build. Set to -1 to never trigger a model rebuild due to data written.
  data-threshold = -1

  # Number of writes into the Serving Layer after which data will be batch uploaded to HDFS.
  writes-between-upload = 10000

  generations = {
    # Number of generations to keep; when exceeded, older ones will be deleted.
    keep = 10
  }

  # Location where all data and work occurs. This can be a local directory in local mode, or an HDFS
  # directory otherwise.
  instance-dir = null

  # Fraction of new input to be used for cross-validation and not model training.
  # Does not have effect on all models yet.
  test-set-fraction = 0

  # The default master to use for Spark runs.
  spark-master= local
}


# Only one of the following *-model values will be used in any given deployment.
# The type of model is specified by setting, for example, model=${als-model}

# als-model
# ALS-based recommender config

als-model = ${model} {

  type = als

  # Default number of model features
  features = 30
  # Default over-fitting parameter, lambda
  lambda = 0.1
  # Default alpha model parameter
  alpha = 1.0

  iterations = {
    # Maximum number of iterations to run, regardless of convergence
    max = 30
    # If average absolute change in reconstructed values is below this, stop iterating
    convergence-threshold = 0.001
  }

  # Configures offline computation of recommendations in the Computation Layer.
  # Does not affect the Serving Layer.
  recommend = {
    # If true, compute recs for all users in the Computation Layer
    compute = false
    # How many recommendations to compute per user
    how-many = 10
  }

  # Configures offline computation of similar items in the Computation Layer.
  # Does not affect the Serving Layer.
  item-similarity = {
    # If true, compute most-similar items for all items in the Computation Layer
    compute = false
    # How many most-similar items to compute for each item
    how-many = 10
  }

  # If true, don't use values as weights, but actually try to reconstruct input values
  # (Reconstruct R matrix in the algorithm, not P
  # Advanced, special-purpose option: don't set this in general.
  reconstruct-r-matrix = false

  # If true, change algorithm's loss function to only include elements that are present in the input
  # That is, don't penalize not reconstructing the implicit 0 elements.
  # Advanced, special-purpose option: don't set this in general.
  loss-ignores-unspecified = false

  # Controls whether model data 'decays' with each generation.
  decay = {
    # New value as decayed fraction of old value; in (0,1]
    factor = 1.0
    # Value below which something is considered to be effectively 0
    zeroThreshold = 0.0
  }

  # Configures location-sensitive hashing. Only applicable if LSH is enabled by setting sample-ratio < 1.
  lsh = {
    # Target fraction of total elements to consider by sampling. LSH is disabled unless this is set < 1.
    # This is used to speed up methods like /recommend at the cost of accuracy.
    sample-ratio = 1.0
    # Number of bits of hash to use
    num-hashes = 20
  }

  # If true, don't remember items associated to each user. Saves memory; recommendations will have
  # items already interacted with though
  no-known-items = false

}

# kmeans-model
# K-Means++ clustering model

kmeans-model = ${model} {

  type = kmeans

  normalize = {
    default-transform = none
    #sparse = true
    #z-transform = ["foo", 7 ]
    #no-transform = [ "abc" ]
    #linear-transform = [ 2, 3, 4 ]
  }

  # How many sketch passes to run
  sketch-iterations = 5
  # How many points to sample on each iteration
  sketch-points = 100

  # How many cross-folds to use during the evaluation of K; two is almost always the right choice.
  cross-folds = 2

  # For the LSH index, how many bits to track. Increasing this number increases the runtime of the
  # iterations but improves the accuracy of the clustering.
  index-bits = 128
  # For the LSH index, the number of points from the index to consider for each point. Increasing this
  # number increases the runtime of the iterations, but improves the accuracy of the clustering.
  index-samples = 32

  # Candidate values of K to consider. K = 1 is always included by default for evaluation purposes.
  k = [2, 5, 10]
  # How many random re-starts to perform for each value of K.
  replications = 3

  init-strategy = "PLUS_PLUS" # One of PLUS_PLUS or RANDOM
  update-strategy = {
    type = MINIBATCH # One of LLOYDS or MINIBATCH
    iterations = 100 # Iterations for the strategy (required for both LLOYDS and MINIBATCH)
    batch-size = 100 # Only used by MINIBATCH: how many points to include in each mini-batch run
  }
  eval-strategy = {
    type = STABLE    # One of THRESHOLD, FIXED, or STABLE
    criterion = vi   # Variation of Information (the alternative is vd, for Van Dongen)
    use = median     # Rule for comparing values of K based on the criterion (the alternative is 'mean')
    #threshold = 0.5 # For type = THRESHOLD, find the best cost solution that had a Variation of Information < threshold
    #k = 10          # For type = FIXED, choose the lowest cost solution for some fixed k
  }
  outliers = {
    compute = true
    mahalanobis = true
  }
}

# rdf-model
# Random decision forest model

rdf-model = ${model} {

  type = rdf

  # Number of trees to build
  num-trees = 20

  # Fraction of features to consider splitting on at each node. For example, if set to 1, then every feature
  # is considered at every node when finding the best split.
  fraction-features-to-try = 0.5

  # Only split nodes when the resulting children have at least this many examples:
  min-node-size = 16

  # Only split nodes when a split can be found that gains at least this much information, in nats (not bits)
  min-info-gain-nats = 0.001

  # How much of the data should each tree learn on, approximately:
  sample-rate = 1.0

  # Maximum number of splits, per feature, to consider at a node.
  max-split-candidates = 8

  # Maximum tree depth
  max-depth = 64

}

# inbound
# Like model above, selects what type of input will be read -- CSV, Hive table, etc.
# Right now only CSV is supported.

inbound = ${csv-input}

# csv-input
# Configuration for CSV text file input

csv-input = {

  type = csv

  # The delimiter for all delimited input. Generally, leave at ","
  delim = ","

  # Names of columns. Either specify a name for all columns of the input, or, column count with num-columns
  # Ex: column-names = [ "foo", "bar", "baz" ]
  column-names = null
  # Set this if column-names has not been set. Columns will take names 0, 1, 2 ...
  num-columns = 0

  # Note below that in lists, columns can be referred to by name, or by 0-indexed position.
  # ["foo", 2, 7] is a valid list.

  # Columns that should be considered an identifier for the datum, if any
  id-columns = []
  # Columns that should be ignored entirely
  ignored-columns = []

  # Specifies which columns are numeric and which are categorical. Specify for all non-ignored columns.
  # Set either numeric-columns, in which case everything else is considered categorical, or set
  # categorical-columns in which case everything else is considered numeric.
  # Set one of the following two:
  #numeric-columns = []
  #categorical-columns = []

  # Which column is the target to be predicted, if applicable.
  target-column = null
}

# Don't set these -- useful for unit testing framework only.
test = {
  integration = false
}
